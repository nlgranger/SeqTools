{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast minibatch sampling\n",
    "\n",
    "In this example, we show how to create a fast minibatch generator, which is typically used in Machine Learning to feed a training routine.\n",
    "It is not the intent of SeqTools to supplant specialized libraries such as tensorflow's [data module](https://www.tensorflow.org/guide/datasets) or [torch.utils.Dataset](https://pytorch.org/docs/stable/data.html), but these might lack simplicity and flexibility for certain usages.\n",
    "Besides, it is absolutly possible to use seqtools at an early stage to connect with these modules.\n",
    "\n",
    "**Note**: As a general guideline, a special care should be taken when using worker based functions along with these libraries.\n",
    "User are advised to become familiar with the behaviour of Python [threads](https://docs.python.org/3/library/threading.html) and [processes](https://docs.python.org/3/library/multiprocessing.html) before using them.\n",
    "\n",
    "## Data samples\n",
    "\n",
    "For this example we consider a set of (X, y) data samples composed of a real vector observation and an integer label.\n",
    "It is common practice to store the data samples by large groups into a few binary dump files.\n",
    "The following script generates some random samples to simulate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "workdir = tempfile.TemporaryDirectory()\n",
    "os.chdir(workdir.name)\n",
    "\n",
    "n_samples = 100000\n",
    "n_classes = 10\n",
    "sample_shape = (248,)\n",
    "chunk_size = 12000\n",
    "\n",
    "# generate reference class centers\n",
    "means = np.random.randn(n_classes, *sample_shape) * 3\n",
    "\n",
    "# generate random class labels\n",
    "labels = np.random.randint(n_classes, size=n_samples)\n",
    "np.save('labels.npy', labels)\n",
    "\n",
    "# generate noisy samples\n",
    "n_chunks = n_samples // chunk_size + (1 if n_samples % chunk_size > 0 else 0)\n",
    "for i in range(n_chunks):\n",
    "    n = min((i + 1) * chunk_size, n_samples) - i * chunk_size\n",
    "    chunk_file = \"data_{:02d}.npy\".format(i)\n",
    "    data = means[labels[i * chunk_size:i * chunk_size + n]] \\\n",
    "        + np.random.randn(n, *sample_shape) * 0.1\n",
    "    np.save(chunk_file, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Now begins the actual data loading.\n",
    "Assuming the dataset is too big to fit in memory, data is read directly from the files and not from memory using memory mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seqtools\n",
    "\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "data_files = sorted(f for f in os.listdir() if f.startswith('data_'))\n",
    "data_chunks = [np.load(f, mmap_mode='r') for f in data_files]\n",
    "data = seqtools.concatenate(data_chunks)\n",
    "\n",
    "assert len(data) == n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate is easy to memorize and does the job, but for that particular case we could also use `data = seqtools.concatenate(data_chunks)` since all of our data chunks (except for the last one) have the same size.\n",
    "\n",
    "Let's now assemble the samples with their labels to facilitate manipulation and split the dataset between training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seqtools.collate([data, labels])\n",
    "train_dataset = dataset[:-10000]\n",
    "test_dataset = dataset[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a simple random minibatch sampler and pass it to `seqtools.load_buffers` to start generating samples with multiple background workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \"\"\"Assembles samples into a minibatch.\"\"\"\n",
    "    batch_data = np.stack([sample[0] for sample in samples])\n",
    "    batch_labels = np.stack([sample[1] for sample in samples])\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "\n",
    "class MinibatchSampler:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __call__(self):\n",
    "        subset = np.random.choice(len(self.dataset), self.batch_size)\n",
    "        samples = list(seqtools.gather(self.dataset, subset))\n",
    "        return collate(samples)\n",
    "\n",
    "\n",
    "sampler = MinibatchSampler(train_dataset, 64)\n",
    "minibatch_iter = seqtools.load_buffers(sampler, max_cached=10, nworkers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minibatch_iter` simply yields minibatches indefinitely by repeatedly calling `sampler` and put the results into buffers which are returned at each iteration.\n",
    "*Please, note that the buffer slots are cyclicly so their content should not be used across iterations.*\n",
    "\n",
    "The sampler is a bit exagerated here and a simple function would suffice in this case:\n",
    "\n",
    "```python\n",
    "def sample_minibatch():\n",
    "    subset = np.random.choice(len(train_dataset), 64)\n",
    "    samples = list(seqtools.gather(train_dataset, subset))\n",
    "    return collate(samples)\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "With the minibatches ready to be used, we create a Gaussian Naive Bayes model and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(5000):\n",
    "    X, y = next(minibatch_iter)\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without multiprocessing to prefetech the minibatches, the training procedure must wait for its input data.\n",
    "In this case, the impact is fairly severe since training itself is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(5000):\n",
    "    X, y = sampler()\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For completeness, we evaluate the accuracy of the results on the testing data.\n",
    "Assuming the testing dataset is also too big, the evaluation proceeeds by small chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_chunks = seqtools.batch(test_dataset, 64, collate_fn=collate)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "t1 = time.time()\n",
    "for X, y in testing_chunks:\n",
    "    predictions.extend(model.predict(X))\n",
    "    targets.extend(y)\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(targets))\n",
    "print(\"Accuracy: {:.0f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
