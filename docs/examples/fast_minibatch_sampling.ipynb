{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast minibatch sampling\n",
    "\n",
    "In this example, we show how to create a fast minibatch generator, which is typically used in Machine Learning to feed a training routine.\n",
    "It is not the intent of SeqTools to supplant specialized libraries such as tensorflow's [data module](https://www.tensorflow.org/guide/datasets) or [torch.utils.Dataset](https://pytorch.org/docs/stable/data.html), but these might lack simplicity and flexibility for certain usages.\n",
    "Besides, it is absolutly possible to use seqtools to provide the inputs for these modules.\n",
    "\n",
    "**Note**: As a general guideline, special care should be taken when using worker based functions along with these libraries.\n",
    "User are advised to become familiar with the behaviour of Python [threads](https://docs.python.org/3/library/threading.html) and [processes](https://docs.python.org/3/library/multiprocessing.html) before using them.\n",
    "\n",
    "## Data samples\n",
    "\n",
    "For this example we consider a set of (X, y) data samples composed of a real vector observation and an integer label.\n",
    "Since it is common practice to store these samples by large groups into a few binary dump files, the following script generates such files to mock a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "workdir = tempfile.TemporaryDirectory()\n",
    "os.chdir(workdir.name)\n",
    "\n",
    "n_samples = 18000\n",
    "n_classes = 10\n",
    "sample_shape = (248,)\n",
    "chunk_size = 5000\n",
    "\n",
    "# generate reference class centers\n",
    "means = np.random.randn(n_classes, *sample_shape) * 3\n",
    "\n",
    "# generate random class labels\n",
    "targets = np.random.randint(n_classes, size=n_samples)\n",
    "np.save('targets.npy', targets)\n",
    "\n",
    "# generate noisy samples\n",
    "n_chunks = n_samples // chunk_size + (1 if n_samples % chunk_size > 0 else 0)\n",
    "for i in range(n_chunks):\n",
    "    n = min((i + 1) * chunk_size, n_samples) - i * chunk_size\n",
    "    chunk_file = \"values_{:02d}.npy\".format(i)\n",
    "    values = means[targets[i * chunk_size:i * chunk_size + n]] \\\n",
    "        + np.random.randn(n, *sample_shape) * 0.1\n",
    "    np.save(chunk_file, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Now begins the actual data loading.\n",
    "Assuming the dataset is too big to fit in memory, memory mapping is used to access the file's content more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seqtools\n",
    "\n",
    "targets = np.load(\"targets.npy\")\n",
    "\n",
    "values_files = sorted(f for f in os.listdir() if f.startswith('values_'))\n",
    "values_chunks = [np.load(f, mmap_mode='r') for f in values_files]\n",
    "values = seqtools.concatenate(values_chunks)\n",
    "\n",
    "assert len(values) == len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate is easy to memorize and does the job, but for that particular case we could also use `values = seqtools.unbatch(values_chunks)` since all of our data chunks (except for the last one) have the same size.\n",
    "\n",
    "Let's now assemble the samples with their targets to facilitate manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seqtools.collate([values, targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split the dataset between training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:-10000]\n",
    "test_dataset = dataset[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, training will be done iteratively using small batches of data sampled from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = np.stack([x for x, _ in batch])\n",
    "    targets = np.stack([y for _, y in batch])\n",
    "    return inputs, targets\n",
    "\n",
    "batches = seqtools.batch(train_dataset, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "With the minibatches ready to be used, we create a Gaussian Naive Bayes model and train over the dataset a 20 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(50):\n",
    "    for inputs, targets in batches:\n",
    "        model.partial_fit(inputs, targets, classes=classes)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is very simple, building the batches actually takes more time than training.\n",
    "While there is not much that can be done to build individual batches faster, prefetching can help by building batches concurrently using multiple cpu cores.\n",
    "SeqTools proposes three different prefetching methods:\n",
    "\n",
    "- `'thread'` has the smallest overhead but only offer true concurrency for specific loads, notably IO bound operations.\n",
    "- `'process'` offers true parallelism but values computed by the workers must be sent back to the main process which incurs serialization costs.\n",
    "- `'sharedmem'` also offers true parellelism, but without serialization thanks to the use of shared memory. However, it only supports item values that implement the buffer protocol such as numpy arrays (for convenience, tuples or dicts of these buffers are also supported).\n",
    "\n",
    "In this example, any of the three methods can work.\n",
    "`'thread'` will be the slowest since IO operations are not critical, whereas `'process'` and `'sharedmem'` should both boost the execution speed compared the serial loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'sharedmem'\n",
    "prefetched_batches = seqtools.prefetch(\n",
    "    batches, method=method, nworkers=2, max_buffered=40)\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(50):\n",
    "    for inputs, targets in prefetched_batches:\n",
    "        model.partial_fit(inputs, targets, classes=classes)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For completeness, we evaluate the accuracy of the results on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = seqtools.batch(test_dataset, batch_size, collate_fn=collate_fn)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "t1 = time.time()\n",
    "for X, y in test_batches:\n",
    "    predictions.extend(model.predict(X))\n",
    "    targets.extend(y)\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(targets))\n",
    "print(\"Accuracy: {:.0f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
