{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast minibatch sampling\n",
    "\n",
    "This example shows how to create minibatches from a dataset, which is found in a Machine Learning pipeline.\n",
    "A SeqTools object can then easily serve as input to [data module](https://www.tensorflow.org/guide/datasets) or [torch.utils.Dataset](https://pytorch.org/docs/stable/data.html).\n",
    "\n",
    "## Data samples\n",
    "\n",
    "For this example we consider a set of (X, y) data samples where X is a real vector observation and y an integer label.\n",
    "\n",
    "The following script generates sample data and stores it into large chunks of `chunk_size` items to mock a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "workdir = tempfile.TemporaryDirectory()\n",
    "os.chdir(workdir.name)\n",
    "\n",
    "n_samples = 18000\n",
    "n_classes = 10\n",
    "sample_shape = (248,)\n",
    "chunk_size = 5000\n",
    "\n",
    "# generate reference class centers\n",
    "means = np.random.randn(n_classes, *sample_shape) * 3\n",
    "\n",
    "# generate random class labels\n",
    "targets = np.random.randint(n_classes, size=n_samples)\n",
    "np.save('targets.npy', targets)\n",
    "\n",
    "# generate noisy samples\n",
    "n_chunks = n_samples // chunk_size + (1 if n_samples % chunk_size > 0 else 0)\n",
    "for i in range(n_chunks):\n",
    "    n = min((i + 1) * chunk_size, n_samples) - i * chunk_size\n",
    "    chunk_file = \"values_{:02d}.npy\".format(i)\n",
    "    values = means[targets[i * chunk_size:i * chunk_size + n]] \\\n",
    "        + np.random.randn(n, *sample_shape) * 0.1\n",
    "    np.save(chunk_file, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Now begins the actual data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seqtools\n",
    "\n",
    "targets = np.load(\"targets.npy\")\n",
    "\n",
    "values_files = sorted(f for f in os.listdir() if f.startswith('values_'))\n",
    "# use mmap if the data cannot fit in memory\n",
    "values_chunks = [np.load(f) for f in values_files]\n",
    "values = seqtools.concatenate(values_chunks)\n",
    "\n",
    "assert len(values) == len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seqtools.concatenate` consolidates the chunks back into a single list of items, but for that particular case we could also use `values = seqtools.unbatch(values_chunks)` because all chunks (except for the last one) have the same size.\n",
    "\n",
    "Let's now assemble the samples with their targets to facilitate manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seqtools.collate([values, targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split the dataset between training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:-10000]\n",
    "test_dataset = dataset[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, training will be done iteratively using small batches of data sampled from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = np.stack([x for x, _ in batch])\n",
    "    targets = np.stack([y for _, y in batch])\n",
    "    return inputs, targets\n",
    "\n",
    "batches = seqtools.batch(train_dataset, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "With the minibatches ready to be used, we create a Gaussian Naive Bayes model and train over the dataset several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 s, sys: 6.46 ms, total: 7.81 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(50):\n",
    "    for inputs, targets in batches:\n",
    "        pass\n",
    "        # model.partial_fit(inputs, targets, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is very simple, building the batches actually takes more time than training.\n",
    "While there is not much that can be done to build individual batches faster, prefetching can help by building batches concurrently using multiple cpu cores.\n",
    "SeqTools proposes two prefetching methods:\n",
    "\n",
    "- `'thread'` has the smallest overhead but only offer true concurrency for specific loads, notably IO bound operations.\n",
    "- `'process'` offers true parallelism but values computed by the workers must be sent back to the main process which incurs serialization costs. For buffers data such as numpy arrays, this can be aleviated by the use of shared memory (`shm_size` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'process'\n",
    "prefetched_batches = seqtools.prefetch(\n",
    "    batches, method=method, nworkers=2, max_buffered=40, shm_size=10 * 1024 ** 2)\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 359 ms, total: 2.91 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(50):\n",
    "    for inputs, targets in prefetched_batches:\n",
    "        pass\n",
    "        # model.partial_fit(inputs, targets, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For completeness, we evaluate the accuracy of the results on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = seqtools.batch(test_dataset, batch_size, collate_fn=collate_fn)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for X, y in test_batches:\n",
    "    predictions.extend(model.predict(X))\n",
    "    targets.extend(y)\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(targets))\n",
    "print(\"Accuracy: {:.0f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seqtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "732a98e1bd2fbf3bf79b1c9dbb0c3d21960821ee38a5c7d6ce6804d1285c688b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
